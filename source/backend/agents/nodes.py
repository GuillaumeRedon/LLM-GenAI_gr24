"""Agent node functions for the LangGraph workflow"""
from agents.state import AgentState
from agents.tools import RAGRetrieverTool
from tools.ollamaChat import create_ollama_chat
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, AIMessage


# Lightweight model for faster agent performance
AGENT_MODEL = "gemma2:2b"


def retrieve_context(state: AgentState) -> AgentState:
    """Node: Retrieve relevant documents from RAG system
    
    This agent node uses the RAG retriever to find documents
    relevant to the user's question.
    """
    question = state["question"]
    
    # Initialize RAG retriever tool
    rag_tool = RAGRetrieverTool()
    
    # Retrieve documents
    docs = rag_tool.retrieve(question, k=6)
    
    # Format documents for context
    formatted_docs = rag_tool.format_docs(docs)
    
    # Store in state
    state["retrieved_docs"] = [formatted_docs]
    
    return state


def generate_answer(state: AgentState) -> AgentState:
    """Node: Generate answer using LLM with retrieved context
    
    This agent node takes the retrieved documents and conversation
    history to generate a contextual answer.
    """
    question = state["question"]
    retrieved_docs = state.get("retrieved_docs", [])
    messages = state.get("messages", [])
    
    # Format conversation history
    conversation_history = []
    for msg in messages:
        if isinstance(msg, HumanMessage):
            conversation_history.append(f"Utilisateur: {msg.content}")
        elif isinstance(msg, AIMessage):
            conversation_history.append(f"Assistant: {msg.content}")
    
    conversation_str = "\n".join(conversation_history)
    
    # Context from retrieved documents
    context = retrieved_docs[0] if retrieved_docs else "Aucun document trouv√©."
    
    llm = create_ollama_chat(model=AGENT_MODEL, temperature=0.3)
    
    # Prompt template
    template = """Tu es un assistant virtuel pour une √©cole. Tu dois r√©pondre √† la derni√®re question de l'utilisateur en t'appuyant sur:
1. Les documents de la base de connaissances ci-dessous
2. L'historique de la conversation pour comprendre le contexte

R√àGLES IMPORTANTES:
- Utilise les informations des documents pour r√©pondre, m√™me si la formulation de la question n'est pas exactement la m√™me que dans les documents
- Si les documents contiennent des informations pertinentes qui peuvent aider √† r√©pondre, utilise-les pour construire ta r√©ponse
- Sois clair et p√©dagogique dans tes explications
- Si vraiment AUCUNE information dans les documents ne peut aider √† r√©pondre (par exemple une question sur la m√©t√©o), dis alors : "Je n'ai pas d'information sur ce sujet dans ma base de connaissances."
- Ne r√©ponds QU'√Ä la derni√®re question pos√©e
- Utilise l'historique pour comprendre le contexte de la conversation

=== DOCUMENTS DE LA BASE DE CONNAISSANCES ===
{context}

=== HISTORIQUE DE LA CONVERSATION ===
{conversation_history}

=== DERNI√àRE QUESTION √Ä R√âPONDRE ===
{question}

R√©ponse de l'assistant:"""
    
    prompt = ChatPromptTemplate.from_template(template)
    
    # Create chain
    chain = prompt | llm
    
    # Generate response
    response = chain.invoke({
        "context": context,
        "conversation_history": conversation_str,
        "question": question
    })
    # Store answer
    state["answer"] = response.content if hasattr(response, 'content') else str(response)
    
    return state


def validate_answer(state: AgentState) -> AgentState:
    """Node: Validate answer quality and relevance
    
    This agent checks if the generated answer:
    1. Uses information from retrieved documents
    2. Answers the actual question
    3. Doesn't hallucinate
    """
    question = state["question"]
    answer = state["answer"]
    retrieved_docs = state.get("retrieved_docs", [])
    
    llm = create_ollama_chat(model=AGENT_MODEL, temperature=0.1)
    
    validation_template = """Tu es un validateur d'IA. Analyse si la r√©ponse est de bonne qualit√©.

QUESTION: {question}

DOCUMENTS DISPONIBLES:
{context}

R√âPONSE G√âN√âR√âE:
{answer}

√âvalue la r√©ponse selon ces crit√®res:
1. La r√©ponse utilise-t-elle les informations des documents? (OUI/NON)
2. La r√©ponse r√©pond-elle vraiment √† la question? (OUI/NON)
3. La r√©ponse contient-elle des informations invent√©es non pr√©sentes dans les documents? (OUI/NON)

R√©ponds UNIQUEMENT par: VALID ou INVALID suivi d'une raison courte.
Format: VALID: [raison] ou INVALID: [raison]"""
    
    prompt = ChatPromptTemplate.from_template(validation_template)
    chain = prompt | llm
    
    validation_result = chain.invoke({
        "question": question,
        "context": retrieved_docs[0] if retrieved_docs else "Aucun document",
        "answer": answer
    })
    
    result_text = validation_result.content if hasattr(validation_result, 'content') else str(validation_result)
    
    # Store validation result
    state["validation"] = result_text
    state["is_valid"] = result_text.strip().startswith("VALID")
    
    print(f"üîç Validation result: {result_text[:100]}...")
    
    return state


def regenerate_answer(state: AgentState) -> AgentState:
    """Node: Regenerate answer with stricter prompt after validation failure
    
    This agent is called when validation fails, using a more
    strict prompt to force document-based answers.
    """
    question = state["question"]
    retrieved_docs = state.get("retrieved_docs", [])
    validation_reason = state.get("validation", "")
    
    llm = create_ollama_chat(model=AGENT_MODEL, temperature=0.3)
    
    strict_template = """ATTENTION: Ta r√©ponse pr√©c√©dente a √©t√© rejet√©e pour: {validation_reason}

Tu DOIS r√©pondre en utilisant UNIQUEMENT les informations ci-dessous.
Si les documents ne contiennent pas d\'information pour r√©pondre, dis clairement:
"Je n\'ai pas d\'information sur ce sujet dans ma base de connaissances."

DOCUMENTS:
{context}

QUESTION:
{question}

R√©ponse (bas√©e UNIQUEMENT sur les documents):"""
    
    prompt = ChatPromptTemplate.from_template(strict_template)
    chain = prompt | llm
    
    response = chain.invoke({
        "context": retrieved_docs[0] if retrieved_docs else "",
        "question": question,
        "validation_reason": validation_reason
    })
    
    state["answer"] = response.content if hasattr(response, 'content') else str(response)
    
    return state
